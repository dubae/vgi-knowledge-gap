{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5003, 0.4997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 진공관 속 양초 사진\n",
    "\n",
    "image = Image.open(\"image/A candle is placed inside a sealed glass vacuum chamber.png\")\n",
    "texts = [\"The light is on inside the vacuum chamber.\", \"The light is off inside the vacuum chamber.\"]\n",
    "\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)  # 어떤 설명이 가장 잘 맞는지 확률로 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 진공관 양초\n",
    "image = Image.open(\"image/A candle is placed inside a sealed glass vacuum chamber.png\")\n",
    "\n",
    "# Multiple text descriptions\n",
    "texts = [\n",
    "    \"A candle is placed inside a sealed glass vacuum chamber\", # prompt\n",
    "    \"A candle is burning in a glass container.\", # logically true, physically true\n",
    "    \"A lit candle is placed inside a transparent glass dome.\", # true, true\n",
    "    \"A flame continues to burn without any air.\", # logically false, physically true\n",
    "    \"A candle is on fire inside a chamber with no oxygen.\", # logically false, physically true\n",
    "    \"A flame can not continue to burn without any air.\" ,# t, f\n",
    "    \"A candle is on fire inside a chamber with oxygen.\", #t,f\n",
    "    \"It is cold inside the vacuum chamber\", #t, f \n",
    "    \"It is hot inside the vacuum chamber\",  #f, t\n",
    "    \"There is a person inside the vacuum chamber\" # f, f\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거울 앞 춤추기\n",
    "image = Image.open(\"image/A person is dancing in front of a mirror.png\")\n",
    "\n",
    "# Multiple text descriptions\n",
    "texts = [\n",
    "    \"A person is dancing in front of a mirror\", # prompt\n",
    "    \"A dancer is facing a mirror during practice\", # logically true, physically true\n",
    "    \"A person is doing something different from a mirror.\", # logically false, physically true\n",
    "    \"A person is doing same thing from a mirror.\", # logically true, physically false\n",
    "    \"Two people are dancing face to face.\", # logically true, physically false\n",
    "    \"Two people are dancing.\",\n",
    "    \"The image of a chicken\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코끼리와 쥐가 시소타는 장면. 근데 쥐 쪽으로 기울어짐\n",
    "image = Image.open(\"image/An elephant and a mouse stand on either side of a seesaw.png\")\n",
    "\n",
    "# Multiple text descriptions\n",
    "texts = [\n",
    "    \"a mouse heavier than an elephant\",   # false\n",
    "    \"an elephant heavier than a mouse\",   # true\n",
    "    \"The seesaw tilted toward the mouse\", # false\n",
    "    \"The seesaw tilted toward the elephant\", # true\n",
    "    \"An elephant and a mouse stand on either side of a seesaw\",  #프롬프트\n",
    "    \"A elephant and a snake are making a swing.\" # f f\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with \"a mouse heavier than an elephant\": 0.3025\n",
      "Similarity with \"an elephant heavier than a mouse\": 0.2958\n",
      "Similarity with \"The seesaw tilted toward the mouse\": 0.3105\n",
      "Similarity with \"The seesaw tilted toward the elephant\": 0.3353\n",
      "Similarity with \"An elephant and a mouse stand on either side of a seesaw\": 0.3706\n",
      "Similarity with \"A elephant and a snake are making a car.\": 0.2796\n"
     ]
    }
   ],
   "source": [
    "#정확도 체크\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess inputs\n",
    "inputs = processor(text=texts, images=[image]*len(texts), return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "# Get features\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(pixel_values=inputs[\"pixel_values\"])  # shape: (N, D)\n",
    "    text_features = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "\n",
    "# Normalize\n",
    "image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "# Cosine similarity: diag(img[i] @ text[i]) for each pair\n",
    "similarities = torch.sum(image_features * text_features, dim=1)\n",
    "\n",
    "# Print result\n",
    "for text, sim in zip(texts, similarities):\n",
    "    print(f\"Similarity with \\\"{text}\\\": {sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9974, 0.0026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[21.8407, 15.8862]], device='cuda:0', grad_fn=<TBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가위바위보 거울 사진(거울과 사람이 다름)\n",
    "\n",
    "image = Image.open(\"image/A person is playing rock-paper-scissors with a mirror.png\")\n",
    "texts = [\"There is a person \", \"The seesaw tilted toward the mouse.\"]\n",
    "\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(probs)  # 어떤 설명이 가장 잘 맞는지 확률로 출력\n",
    "print(logits_per_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
